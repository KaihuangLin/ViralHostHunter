from keras import Input, Model
from keras.layers import Layer, GlobalAveragePooling2D, Dense, multiply
from keras.layers import Conv2D, BatchNormalization, ReLU, Dropout, Concatenate
from keras.layers import MaxPooling2D, Flatten, Activation


class SELayer(Layer):
    def __init__(self, input_channel, r=16):
        super(SELayer, self).__init__()
        self.avg_pool = GlobalAveragePooling2D()
        self.dense_1 = Dense(units=input_channel//r, activation="relu", use_bias=False)
        self.dense_2 = Dense(units=input_channel, activation="sigmoid", use_bias=False)

    def call(self, x):
        y = self.avg_pool(x)
        y = self.dense_1(y)
        y = self.dense_2(y)
        return multiply([x, y])

    def get_config(self):
        config = super(SELayer, self).get_config()
        return config


def lysin_species_model():
    input_1 = Input(shape=(1000, 20, 1))
    # subnetwork 1
    conv2d_11 = Conv2D(128, (4, 20))(input_1)
    batch_normalization_11 = BatchNormalization()(conv2d_11)
    relu_11 = ReLU()(batch_normalization_11)
    se_11 = SELayer(input_channel=128)(relu_11)
    dropout_11 = Dropout(rate=0.1)(se_11)
    conv2d_12 = Conv2D(128, (4, 1))(dropout_11)
    batch_normalization_12 = BatchNormalization()(conv2d_12)
    relu_12 = ReLU()(batch_normalization_12)
    se_12 = SELayer(input_channel=128)(relu_12)
    dropout_12 = Dropout(rate=0.1)(se_12)
    conv2d_13 = Conv2D(128, (16, 1))(dropout_12)
    batch_normalization_13 = BatchNormalization()(conv2d_13)
    relu_13 = ReLU()(batch_normalization_13)
    se_13 = SELayer(input_channel=128)(relu_13)
    dropout_13 = Dropout(rate=0.1)(se_13)
    # subnetwork 2
    conv2d_21 = Conv2D(128, (12, 20))(input_1)
    batch_normalization_21 = BatchNormalization()(conv2d_21)
    relu_21 = ReLU()(batch_normalization_21)
    se_21 = SELayer(input_channel=128)(relu_21)
    dropout_21 = Dropout(rate=0.1)(se_21)
    conv2d_22 = Conv2D(128, (8, 1))(dropout_21)
    batch_normalization_22 = BatchNormalization()(conv2d_22)
    relu_22 = ReLU()(batch_normalization_22)
    se_22 = SELayer(input_channel=128)(relu_22)
    dropout_22 = Dropout(rate=0.1)(se_22)
    conv2d_23 = Conv2D(128, (4, 1))(dropout_22)
    batch_normalization_23 = BatchNormalization()(conv2d_23)
    relu_23 = ReLU()(batch_normalization_23)
    se_23 = SELayer(input_channel=128)(relu_23)
    dropout_23 = Dropout(rate=0.1)(se_23)
    # subnetwork 3
    conv2d_31 = Conv2D(128, (16, 20))(input_1)
    batch_normalization_31 = BatchNormalization()(conv2d_31)
    relu_31 = ReLU()(batch_normalization_31)
    se_31 = SELayer(input_channel=128)(relu_31)
    dropout_31 = Dropout(rate=0.1)(se_31)
    conv2d_32 = Conv2D(128, (4, 1))(dropout_31)
    batch_normalization_32 = BatchNormalization()(conv2d_32)
    relu_32 = ReLU()(batch_normalization_32)
    se_32 = SELayer(input_channel=128)(relu_32)
    dropout_32 = Dropout(rate=0.1)(se_32)
    conv1d_33 = Conv2D(128, (4, 1))(dropout_32)
    batch_normalization_33 = BatchNormalization()(conv1d_33)
    relu_33 = ReLU()(batch_normalization_33)
    se_33 = SELayer(input_channel=128)(relu_33)
    dropout_33 = Dropout(rate=0.1)(se_33)
    # cat
    concatenate = Concatenate(axis=3)([dropout_13, dropout_23, dropout_33])
    conv2d_1 = Conv2D(384, (1, 1))(concatenate)
    batch_normalization_1 = BatchNormalization()(conv2d_1)
    relu_1 = ReLU()(batch_normalization_1)
    se_1 = SELayer(input_channel=384)(relu_1)
    maxpooling2d_1 = MaxPooling2D((979, 1))(se_1)
    # dense
    flatten = Flatten()(maxpooling2d_1)
    dense_1 = Dense(512)(flatten)
    batch_normalization_2 = BatchNormalization()(dense_1)
    relu_2 = ReLU()(batch_normalization_2)
    dense_2 = Dense(47)(relu_2)
    batch_normalization_3 = BatchNormalization()(dense_2)
    activation_1 = Activation(activation='softmax')(batch_normalization_3)

    model = Model(inputs=input_1, outputs=activation_1)
    return model

def lysin_genus_model():
    input_1 = Input(shape=(1000, 20, 1))
    # subnetwork 1
    conv2d_11 = Conv2D(128, (4, 20))(input_1)
    batch_normalization_11 = BatchNormalization()(conv2d_11)
    relu_11 = ReLU()(batch_normalization_11)
    se_11 = SELayer(input_channel=128)(relu_11)
    dropout_11 = Dropout(rate=0.1)(se_11)
    conv2d_12 = Conv2D(128, (4, 1))(dropout_11)
    batch_normalization_12 = BatchNormalization()(conv2d_12)
    relu_12 = ReLU()(batch_normalization_12)
    se_12 = SELayer(input_channel=128)(relu_12)
    dropout_12 = Dropout(rate=0.1)(se_12)
    conv2d_13 = Conv2D(128, (16, 1))(dropout_12)
    batch_normalization_13 = BatchNormalization()(conv2d_13)
    relu_13 = ReLU()(batch_normalization_13)
    se_13 = SELayer(input_channel=128)(relu_13)
    dropout_13 = Dropout(rate=0.1)(se_13)
    # subnetwork 2
    conv2d_21 = Conv2D(128, (12, 20))(input_1)
    batch_normalization_21 = BatchNormalization()(conv2d_21)
    relu_21 = ReLU()(batch_normalization_21)
    se_21 = SELayer(input_channel=128)(relu_21)
    dropout_21 = Dropout(rate=0.1)(se_21)
    conv2d_22 = Conv2D(128, (8, 1))(dropout_21)
    batch_normalization_22 = BatchNormalization()(conv2d_22)
    relu_22 = ReLU()(batch_normalization_22)
    se_22 = SELayer(input_channel=128)(relu_22)
    dropout_22 = Dropout(rate=0.1)(se_22)
    conv2d_23 = Conv2D(128, (4, 1))(dropout_22)
    batch_normalization_23 = BatchNormalization()(conv2d_23)
    relu_23 = ReLU()(batch_normalization_23)
    se_23 = SELayer(input_channel=128)(relu_23)
    dropout_23 = Dropout(rate=0.1)(se_23)
    # subnetwork 3
    conv2d_31 = Conv2D(128, (16, 20))(input_1)
    batch_normalization_31 = BatchNormalization()(conv2d_31)
    relu_31 = ReLU()(batch_normalization_31)
    se_31 = SELayer(input_channel=128)(relu_31)
    dropout_31 = Dropout(rate=0.1)(se_31)
    conv2d_32 = Conv2D(128, (4, 1))(dropout_31)
    batch_normalization_32 = BatchNormalization()(conv2d_32)
    relu_32 = ReLU()(batch_normalization_32)
    se_32 = SELayer(input_channel=128)(relu_32)
    dropout_32 = Dropout(rate=0.1)(se_32)
    conv1d_33 = Conv2D(128, (4, 1))(dropout_32)
    batch_normalization_33 = BatchNormalization()(conv1d_33)
    relu_33 = ReLU()(batch_normalization_33)
    se_33 = SELayer(input_channel=128)(relu_33)
    dropout_33 = Dropout(rate=0.1)(se_33)
    # cat
    concatenate = Concatenate(axis=3)([dropout_13, dropout_23, dropout_33])
    conv2d_1 = Conv2D(384, (1, 1))(concatenate)
    batch_normalization_1 = BatchNormalization()(conv2d_1)
    relu_1 = ReLU()(batch_normalization_1)
    se_1 = SELayer(input_channel=384)(relu_1)
    maxpooling2d_1 = MaxPooling2D((979, 1))(se_1)
    # dense
    flatten = Flatten()(maxpooling2d_1)
    dense_1 = Dense(512)(flatten)
    batch_normalization_2 = BatchNormalization()(dense_1)
    relu_2 = ReLU()(batch_normalization_2)
    dense_2 = Dense(42)(relu_2)
    batch_normalization_3 = BatchNormalization()(dense_2)
    activation_1 = Activation(activation='softmax')(batch_normalization_3)

    model = Model(inputs=input_1, outputs=activation_1)
    return model

def lysin_family_model():
    input_1 = Input(shape=(1000, 20, 1))
    # subnetwork 1
    conv2d_11 = Conv2D(128, (4, 20))(input_1)
    batch_normalization_11 = BatchNormalization()(conv2d_11)
    relu_11 = ReLU()(batch_normalization_11)
    se_11 = SELayer(input_channel=128)(relu_11)
    dropout_11 = Dropout(rate=0.1)(se_11)
    conv2d_12 = Conv2D(128, (4, 1))(dropout_11)
    batch_normalization_12 = BatchNormalization()(conv2d_12)
    relu_12 = ReLU()(batch_normalization_12)
    se_12 = SELayer(input_channel=128)(relu_12)
    dropout_12 = Dropout(rate=0.1)(se_12)
    conv2d_13 = Conv2D(128, (16, 1))(dropout_12)
    batch_normalization_13 = BatchNormalization()(conv2d_13)
    relu_13 = ReLU()(batch_normalization_13)
    se_13 = SELayer(input_channel=128)(relu_13)
    dropout_13 = Dropout(rate=0.1)(se_13)
    # subnetwork 2
    conv2d_21 = Conv2D(128, (12, 20))(input_1)
    batch_normalization_21 = BatchNormalization()(conv2d_21)
    relu_21 = ReLU()(batch_normalization_21)
    se_21 = SELayer(input_channel=128)(relu_21)
    dropout_21 = Dropout(rate=0.1)(se_21)
    conv2d_22 = Conv2D(128, (8, 1))(dropout_21)
    batch_normalization_22 = BatchNormalization()(conv2d_22)
    relu_22 = ReLU()(batch_normalization_22)
    se_22 = SELayer(input_channel=128)(relu_22)
    dropout_22 = Dropout(rate=0.1)(se_22)
    conv2d_23 = Conv2D(128, (4, 1))(dropout_22)
    batch_normalization_23 = BatchNormalization()(conv2d_23)
    relu_23 = ReLU()(batch_normalization_23)
    se_23 = SELayer(input_channel=128)(relu_23)
    dropout_23 = Dropout(rate=0.1)(se_23)
    # subnetwork 3
    conv2d_31 = Conv2D(128, (16, 20))(input_1)
    batch_normalization_31 = BatchNormalization()(conv2d_31)
    relu_31 = ReLU()(batch_normalization_31)
    se_31 = SELayer(input_channel=128)(relu_31)
    dropout_31 = Dropout(rate=0.1)(se_31)
    conv2d_32 = Conv2D(128, (4, 1))(dropout_31)
    batch_normalization_32 = BatchNormalization()(conv2d_32)
    relu_32 = ReLU()(batch_normalization_32)
    se_32 = SELayer(input_channel=128)(relu_32)
    dropout_32 = Dropout(rate=0.1)(se_32)
    conv1d_33 = Conv2D(128, (4, 1))(dropout_32)
    batch_normalization_33 = BatchNormalization()(conv1d_33)
    relu_33 = ReLU()(batch_normalization_33)
    se_33 = SELayer(input_channel=128)(relu_33)
    dropout_33 = Dropout(rate=0.1)(se_33)
    # cat
    concatenate = Concatenate(axis=3)([dropout_13, dropout_23, dropout_33])
    conv2d_1 = Conv2D(384, (1, 1))(concatenate)
    batch_normalization_1 = BatchNormalization()(conv2d_1)
    relu_1 = ReLU()(batch_normalization_1)
    se_1 = SELayer(input_channel=384)(relu_1)
    maxpooling2d_1 = MaxPooling2D((979, 1))(se_1)
    # dense
    flatten = Flatten()(maxpooling2d_1)
    dense_1 = Dense(512)(flatten)
    batch_normalization_2 = BatchNormalization()(dense_1)
    relu_2 = ReLU()(batch_normalization_2)
    dense_2 = Dense(37)(relu_2)
    batch_normalization_3 = BatchNormalization()(dense_2)
    activation_1 = Activation(activation='softmax')(batch_normalization_3)

    model = Model(inputs=input_1, outputs=activation_1)
    return model
